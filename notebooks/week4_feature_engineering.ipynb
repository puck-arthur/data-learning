{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3F7E36A1lsnFfkQfwJkXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puck-arthur/data-learning/blob/main/notebooks/week4_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 4 – Feature Engineering & Pipelines\n",
        "\n",
        "## 1. Dataset Selection & Notebook Setup\n",
        "- Continue with the Student Depression dataset (so you can compare against Week 3).\n",
        "- Create a new Colab notebook named `week04_feature_engineering.ipynb` and load your cleaned DataFrame.\n",
        "\n",
        "## 2. Exploratory Data Analysis Refresh\n",
        "- Audit missing values and decide whether to drop, impute, or introduce a “Missing” category.\n",
        "- Check for outliers on all numeric features using boxplots or IQR rules.\n",
        "- Recompute your correlation heatmap to see how original and newly created features relate to the target.\n",
        "\n",
        "## 3. Feature Engineering\n",
        "- Add interaction terms (e.g. study_hours × sleep_hours, social_media_hours × study_hours).\n",
        "- Create polynomial features to capture non-linear relationships (e.g. degree=2 on key numerics).\n",
        "- Bin continuous variables into meaningful categories (low/medium/high).\n",
        "- (Optional) Extract any text or timestamp features available (word counts, time-of-day, etc.).\n",
        "\n",
        "## 4. Preprocessing & Pipelines\n",
        "- Build a numeric pipeline that imputes missing values and scales features.\n",
        "- Build an ordinal pipeline that imputes and encodes ordered categories.\n",
        "- Build a nominal pipeline that imputes and one-hot encodes nominal variables.\n",
        "- Build a custom feature pipeline that applies your interaction and polynomial transformations.\n",
        "- Combine all pipelines into a ColumnTransformer, assigning each pipeline to its column group.\n",
        "- Wrap the ColumnTransformer and your chosen estimator in a top-level Pipeline.\n",
        "\n",
        "## 5. Cross-Validation & Evaluation\n",
        "- Use cross_val_score with 5-fold CV and the “f1_macro” metric on your pipeline.\n",
        "- Compare the mean and standard deviation of CV scores against your Week 3 baseline.\n",
        "\n",
        "## 6. Hyperparameter Tuning\n",
        "- Define a parameter grid that includes both model hyperparameters (e.g. tree depth, n_estimators) and feature-engineering choices (e.g. polynomial degree, whether to include interactions).\n",
        "- Run GridSearchCV with 5-fold CV and “f1_macro” scoring.\n",
        "- Record best_params_ and best_score_ from the search.\n",
        "\n",
        "## 7. Final Evaluation & Diagnostics\n",
        "- Fit the best estimator on the entire training set.\n",
        "- Evaluate on the test set: accuracy, precision/recall/F1-macro, ROC-AUC, and confusion matrix.\n",
        "- Generate and inspect a confusion matrix and ROC curve for your final model."
      ],
      "metadata": {
        "id": "Y9UkHLFP2VPF"
      }
    }
  ]
}